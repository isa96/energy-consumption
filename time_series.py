# -*- coding: utf-8 -*-
"""TIME SERIES_DICODING_FELIX PRATAMASAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oMaVgHqsrlaTNouoQZJsF5v9-jZADiil

# NAMA: FELIX PRATAMASAN
# EMAIL: felixpratama242@gmail.com
"""

import tensorflow as tf
from tensorflow.keras.layers import *
from tensorflow.keras.models import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import zipfile
from google.colab import files
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

"""# Download Dataset from Kaggle"""

#you need to create your own API first in kaggle
#upload kaggle json

files.upload()

#create a kaggle folder
!mkdir ~/.kaggle

#copy kaggle.json to folder created
!cp kaggle.json ~/.kaggle/

#permission for the json to act
! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d ranja7/electricity-consumption

"""# Read Dataset"""

test_local_zip = '/content/electricity-consumption.zip'
zip_ref = zipfile.ZipFile(test_local_zip, 'r')
zip_ref.extractall('/content/dataset')

zip_ref.close()

data = pd.read_csv('/content/dataset/daily_consumption.csv')

data.head()

"""# Exploratory Data Analysist"""

data.isnull().sum()

data = data.dropna()

data.isnull().sum()

data.info()

print('Total Data:{} \nTotal Columns: {}'.format(data.shape[0], data.shape[1]))

timestamp= data['Date']
value = data['Energy Consumption (kWh)']

plt.figure(figsize=(20,15))
plt.plot(timestamp, value)
plt.title('Energy Consumption', fontsize=20)

"""# Preprocessing Data"""

reshape_value = value.values.reshape(-1,1)
scaler = MinMaxScaler()
scaler.fit(reshape_value)
normalized_value = scaler.transform(reshape_value)

x_train, x_test, y_train, y_test = train_test_split(normalized_value, timestamp, train_size=0.8, random_state=42)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
    series = tf.expand_dims(series, axis=-1)
    ds = tf.data.Dataset.from_tensor_slices(series)
    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda w: w.batch(window_size + 1))
    ds = ds.shuffle(shuffle_buffer)
    ds = ds.map(lambda w: (w[:-1], w[-1:]))
    return ds.batch(batch_size).prefetch(1)

train_set = windowed_dataset(x_train, window_size=64, batch_size=256, shuffle_buffer=1000)

"""# Build Model"""

max_val = normalized_value.max()
min_val = normalized_value.min()

print('maximum value: {}\nminimum value: {}'.format(max_val, min_val))

threshold = (max_val - min_val) * 10/100

print('Minimum MAE from this data: {}'.format(threshold))

model = Sequential([
    Conv1D(filters=64, kernel_size=5,
                          strides=1, padding='same',
                          activation='relu',
                          input_shape=[None, 1]),
    Bidirectional(LSTM(64, return_sequences=True)),
    Bidirectional(LSTM(128, return_sequences=True)),
    Bidirectional(LSTM(256)),
    Dense(32, activation="relu"),
    Dense(16, activation="relu"),
    Dense(1),
])

model.summary()

callbacks = EarlyStopping(monitor = 'loss',
                        patience = 3,
                        verbose = 1)

optimizer = tf.keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9)

model.compile(loss=tf.keras.losses.Huber(),
              optimizer=optimizer,
              metrics=["mae"])

history = model.fit(train_set,epochs=20, callbacks=[callbacks], verbose=1)

# Plot the training and validation accuracies for each epoch

mae = history.history['mae']
loss = history.history['loss']

epochs = range(len(mae))

plt.plot(epochs, mae, 'r', label='mae')
plt.plot(epochs, loss, 'b', label='loss')
plt.title('mae and loss')
plt.legend(loc=0)


plt.show()